\section{Value-Based Deep Reinforcement Learning}

\begin{problem}{Linear Value Function Approximation}
We wish to estimate the state-value function $V(s)$ using linear function approximation: $V(s) \approx \hat{v}(s, \mathbf{w}) = \mathbf{w}^\top \mathbf{x}(s)$, where $\mathbf{w}$ is a weight vector and $\mathbf{x}(s)$ is a feature vector representing state $s$.

We use the TD(0) algorithm with semi-gradient descent. The update rule is:
\[ \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha [R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t)] \nabla \hat{v}(S_t, \mathbf{w}_t) \]

**Given:**
\begin{itemize}
    \item Discount factor $\gamma = 0.9$, Step size $\alpha = 0.1$.
    \item Current weights $\mathbf{w}_t = [0.5, -0.2, 0.1]^\top$.
    \item Current state $S_t$ has features $\mathbf{x}(S_t) = [1, 0, 2]^\top$.
    \item Next state $S_{t+1}$ has features $\mathbf{x}(S_{t+1}) = [1, 1, 0]^\top$.
    \item Reward received $R_{t+1} = 2.0$.
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item Calculate the estimated value of the current state $\hat{v}(S_t, \mathbf{w}_t)$.
    \item Calculate the estimated value of the next state $\hat{v}(S_{t+1}, \mathbf{w}_t)$.
    \item Calculate the TD Error $\delta_t$.
    \item Calculate the updated weight vector $\mathbf{w}_{t+1}$.
\end{enumerate}
\end{problem}

\begin{solution}
\textbf{(a) Estimate $\hat{v}(S_t, \mathbf{w}_t)$}
\[ \hat{v}(S_t, \mathbf{w}_t) = \mathbf{w}_t^\top \mathbf{x}(S_t) = (0.5)(1) + (-0.2)(0) + (0.1)(2) \]
\[ = 0.5 + 0 + 0.2 = 0.7 \]

\textbf{(b) Estimate $\hat{v}(S_{t+1}, \mathbf{w}_t)$}
\[ \hat{v}(S_{t+1}, \mathbf{w}_t) = \mathbf{w}_t^\top \mathbf{x}(S_{t+1}) = (0.5)(1) + (-0.2)(1) + (0.1)(0) \]
\[ = 0.5 - 0.2 + 0 = 0.3 \]

\textbf{(c) Calculate TD Error $\delta_t$}
The TD target is $R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t)$.
\[ \text{Target} = 2.0 + 0.9(0.3) = 2.0 + 0.27 = 2.27 \]
\[ \delta_t = \text{Target} - \hat{v}(S_t, \mathbf{w}_t) = 2.27 - 0.7 = 1.57 \]

\textbf{(d) Update Weights $\mathbf{w}_{t+1}$}
For linear approximation, the gradient is simply the feature vector: $\nabla \hat{v}(S_t, \mathbf{w}_t) = \mathbf{x}(S_t) = [1, 0, 2]^\top$.
\[ \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \mathbf{x}(S_t) \]
\[ \mathbf{w}_{t+1} = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.1 \end{bmatrix} + 0.1(1.57) \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix} \]
\[ \mathbf{w}_{t+1} = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.1 \end{bmatrix} + \begin{bmatrix} 0.157 \\ 0 \\ 0.314 \end{bmatrix} \]
\[ \mathbf{w}_{t+1} = \begin{bmatrix} 0.657 \\ -0.2 \\ 0.414 \end{bmatrix} \]
\end{solution}

\newpage

\begin{problem}{Deep Q-Network (DQN) Backpropagation}
Consider a simple Deep Q-Network with one hidden layer.
\begin{center}
\begin{tikzpicture}[x=1.5cm, y=1.5cm]
    % Input Layer
    \node[netnode] (i1) at (0, 1) {$x_1$};
    \node[netnode] (i2) at (0, 0) {$x_2$};
    \node[above] at (0, 1.4) {Input};

    % Hidden Layer (ReLU)
    \node[netnode] (h1) at (2, 1) {$h_1$};
    \node[netnode] (h2) at (2, 0) {$h_2$};
    \node[above] at (2, 1.4) {Hidden (ReLU)};

    % Output Layer (Linear)
    \node[netnode] (o1) at (4, 1) {$Q(s, a_1)$};
    \node[netnode] (o2) at (4, 0) {$Q(s, a_2)$};
    \node[above] at (4, 1.4) {Output};

    % Connections
    \draw[arrow] (i1) -- (h1); \draw[arrow] (i1) -- (h2);
    \draw[arrow] (i2) -- (h1); \draw[arrow] (i2) -- (h2);

    \draw[arrow] (h1) -- node[above, font=\small] {$w_{11}$} (o1);
    \draw[arrow] (h1) -- (o2);
    \draw[arrow] (h2) -- node[below, font=\small] {$w_{21}$} (o1);
    \draw[arrow] (h2) -- (o2);
\end{tikzpicture}
\end{center}

\textbf{Scenario:}
\begin{itemize}
    \item Input state $s = [1, -1]$.
    \item Hidden layer weights (Input $\to$ Hidden) are fixed for this step.
    \item The computed hidden layer activations (after ReLU) are $h = [0.8, 0.5]$.
    \item Weights connecting Hidden to Output node 1 ($Q(s, a_1)$) are $\mathbf{w}_{out, 1} = [w_{11}, w_{21}]^\top = [1.5, -0.5]$.
    \item Bias terms are 0.
\end{itemize}

The agent took action $a_1$.
The environment returned reward $R = 1.0$ and next state $s'$.
The max Q-value calculated from the target network for the next state is $\max_{a'} Q(s', a'; \theta^-) = 2.0$.
Discount factor $\gamma = 0.9$.

\begin{enumerate}[label=(\alph*)]
    \item Calculate the predicted Q-value $Q(s, a_1)$.
    \item Calculate the TD Target $Y$.
    \item Calculate the Loss $L = \frac{1}{2}(Y - Q(s, a_1))^2$.
    \item Calculate the gradient of the Loss with respect to the weight $w_{11}$.
\end{enumerate}
\end{problem}

\begin{solution}
\textbf{(a) Predicted Q-value $Q(s, a_1)$}
The output is a linear combination of the hidden units:
\[ Q(s, a_1) = w_{11} h_1 + w_{21} h_2 \]
Given $h = [0.8, 0.5]$ and $\mathbf{w}_{out, 1} = [1.5, -0.5]$:
\[ Q(s, a_1) = (1.5)(0.8) + (-0.5)(0.5) \]
\[ = 1.2 - 0.25 = 0.95 \]

\textbf{(b) TD Target $Y$}
\[ Y = R + \gamma \max_{a'} Q(s', a'; \theta^-) \]
\[ Y = 1.0 + 0.9(2.0) = 1.0 + 1.8 = 2.8 \]

\textbf{(c) Loss $L$}
\[ L = \frac{1}{2}(Y - Q(s, a_1))^2 \]
\[ L = \frac{1}{2}(2.8 - 0.95)^2 = \frac{1}{2}(1.85)^2 \]
\[ L = \frac{1}{2}(3.4225) = 1.71125 \]

\textbf{(d) Gradient $\frac{\partial L}{\partial w_{11}}$}
Using the Chain Rule:
\[ \frac{\partial L}{\partial w_{11}} = \frac{\partial L}{\partial Q} \cdot \frac{\partial Q}{\partial w_{11}} \]

First part (Error term):
\[ \frac{\partial L}{\partial Q} = -(Y - Q(s, a_1)) = -(1.85) = -1.85 \]
(Note: commonly written as $(Q - Y)$ in gradient descent, but derivative of $\frac{1}{2}(Y-Q)^2$ wrt $Q$ is $-(Y-Q)$).

Second part (Input to weight):
\[ Q = w_{11} h_1 + w_{21} h_2 \implies \frac{\partial Q}{\partial w_{11}} = h_1 = 0.8 \]

Combine:
\[ \frac{\partial L}{\partial w_{11}} = (-1.85) \cdot (0.8) = -1.48 \]

\textit{Note: In a gradient descent update $w \leftarrow w - \alpha \nabla$, we would move the weight in the direction opposite to this gradient (i.e., increase the weight, which makes sense because our prediction 0.95 was lower than the target 2.8).}
\end{solution}

\newpage

\begin{problem}{Double DQN vs Standard DQN (Overestimation Bias)}
In Q-Learning and Standard DQN, the maximization step $\max_{a'} Q(s', a')$ can lead to overestimation of action values. Double DQN (DDQN) addresses this by decoupling action selection from action evaluation.

Consider a state $S'$ where the agent has 3 possible actions $\{a_1, a_2, a_3\}$.
We have two networks:
\begin{itemize}
    \item \textbf{Online Network ($\theta$):} Used to select actions.
    \item \textbf{Target Network ($\theta'$):} Used to evaluate values.
\end{itemize}

The Q-values estimated by both networks for state $S'$ are:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Action & $Q(S', a; \theta)$ (Online) & $Q(S', a; \theta')$ (Target) \\
\hline
$a_1$ & 2.5 & 2.1 \\
$a_2$ & 3.0 & 1.5 \\
$a_3$ & 2.8 & 2.9 \\
\hline
\end{tabular}
\end{center}

Assume Reward $R = 1.0$ and $\gamma = 0.9$.

\begin{enumerate}[label=(\alph*)]
    \item Calculate the TD Target $Y_{DQN}$ using the \textbf{Standard DQN} formula.
    \item Calculate the TD Target $Y_{DDQN}$ using the \textbf{Double DQN} formula.
    \item Which method provides a lower estimate in this case? Explain why this is generally desirable.
\end{enumerate}
\end{problem}

\begin{solution}
\textbf{(a) Standard DQN Target}
Standard DQN uses the Target Network for both selection and evaluation (or historically just one network, but in modern DQN it uses the Target net for the max operator):
\[ Y_{DQN} = R + \gamma \max_{a} Q(S', a; \theta') \]
Looking at the Target Network column:
Values are $\{2.1, 1.5, 2.9\}$.
Max value is $2.9$ (for action $a_3$).
\[ Y_{DQN} = 1.0 + 0.9(2.9) = 1.0 + 2.61 = 3.61 \]

\textbf{(b) Double DQN Target}
Double DQN selects the best action using the \textit{Online} network, but evaluates its value using the \textit{Target} network.
\[ Y_{DDQN} = R + \gamma Q(S', \underset{a}{\text{argmax }} Q(S', a; \theta); \theta') \]

Step 1: Find best action using Online Network $\theta$.
Values are $\{2.5, 3.0, 2.8\}$.
Best action is $a^* = a_2$ (value 3.0).

Step 2: Evaluate this action $a_2$ using Target Network $\theta'$.
Look up $a_2$ in the Target column: $Q(S', a_2; \theta') = 1.5$.

Step 3: Calculate Target.
\[ Y_{DDQN} = 1.0 + 0.9(1.5) = 1.0 + 1.35 = 2.35 \]

\textbf{(c) Comparison}
$Y_{DQN} = 3.61$ vs $Y_{DDQN} = 2.35$.
Double DQN provides a significantly lower estimate.
This is desirable because noise in the estimates often causes the "max" operator to pick an action that is artificially high (overestimated) in the target network. By forcing the action choice to agree with the Online network (which might value a different action highly), we reduce the likelihood of propagating these maximum biases. In this specific example, the Online network thinks $a_2$ is best, but the Target network thinks $a_2$ is actually quite poor (1.5). Standard DQN would have ignored this disagreement and just taken the max of the Target (2.9), potentially training on a hallucinated high value.
\end{solution}

\newpage

\begin{problem}{Dueling DQN Architecture}
The Dueling DQN architecture separates the value of state $V(s)$ from the advantage of actions $A(s, a)$.
The final Q-value is combined using the formula:
\[ Q(s, a) = V(s) + \left( A(s, a) - \frac{1}{|A|} \sum_{a'} A(s, a') \right) \]

Suppose for a state $s$ with 3 actions $\{a_1, a_2, a_3\}$, the network streams output the following:
\begin{itemize}
    \item Value stream output: $V(s) = 10.0$
    \item Advantage stream outputs: $A(s, a_1) = 2.0$, $A(s, a_2) = -1.0$, $A(s, a_3) = 5.0$.
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item Calculate the mean advantage $\bar{A}$.
    \item Calculate the final Q-values for all three actions: $Q(s, a_1)$, $Q(s, a_2)$, $Q(s, a_3)$.
    \item Verify that $V(s) \approx \frac{1}{|A|} \sum Q(s, a)$ is NOT necessarily true, but rather that the relative rank is preserved.
    \item What is the Q-value of the optimal action in this state?
\end{enumerate}
\end{problem}

\begin{solution}
\textbf{(a) Mean Advantage}
\[ \bar{A} = \frac{1}{3} (2.0 + (-1.0) + 5.0) = \frac{1}{3}(6.0) = 2.0 \]

\textbf{(b) Final Q-values}
Formula: $Q(s, a) = 10.0 + (A(s, a) - 2.0)$.

For $a_1$:
\[ Q(s, a_1) = 10.0 + (2.0 - 2.0) = 10.0 + 0 = 10.0 \]

For $a_2$:
\[ Q(s, a_2) = 10.0 + (-1.0 - 2.0) = 10.0 - 3.0 = 7.0 \]

For $a_3$:
\[ Q(s, a_3) = 10.0 + (5.0 - 2.0) = 10.0 + 3.0 = 13.0 \]

\textbf{(c) Verification}
The sum of advantages is forced to be zero in the aggregation.
Note that the Q-values are $10, 7, 13$. The optimal action is $a_3$ (13). The advantages were $2, -1, 5$ (optimal $a_3$). The rank is preserved.

\textbf{(d) Optimal Q-value}
$\max_a Q(s, a) = 13.0$.
\end{solution}

\newpage
